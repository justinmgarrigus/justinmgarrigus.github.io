<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Justin Garrigus: Research</title>
    <link rel="stylesheet" href="../style.css">
    <meta name="robots" content="noindex"> 
  </head>
  <body style="webpage">
	
    <div class="main-area"> 
      <div class="title-area"> 
        <p class="title-text">Justin Garrigus: Research</p>
        <p class="subtitle-text">
          Master's Student in Computer Science, University of North Texas
        </p>
      </div> 
    
      <div class="main-content">
      
        <div class="main-section"> 
          <p class="main-text"> 
            This section describes specific research projects I've worked on
            which were made with the intent of being <i>published</i> or 
            otherwise <i>presented</i> in some academic format. They 
            involved collaborating with professors, PhD students, and 
            undergraduates to create verifyable results that further the 
            academic domain.
          </p>
          
          <p class="main-text"> 
            <table class="main-table"> 
              <tr>
                <th>Title</th>
                <th>Date</th>
                <th>Type</th>
                <th>Status</th>
              </tr>
              <tr> 
                <td>Reducing the Amount of Duplicated Values in the GPU L2
                  Cache during Lowered Convolution</td> 
                <td>April&nbsp;2023 to current</td> 
                <td>Paper, main&nbsp;author</td> 
                <td>In progress</td> 
              <tr>
                <td>Using Transfer Learning and Supercells to Improve Graph 
                  Neural Network Performance in Formation Energy Predictions
                  </td>
                <td>May&nbsp;2024 to current</td> 
                <td>Paper, main&nbsp;author</td>
                <td>In progress</td> 
              </tr> 
              <tr>
                <td>Applying Transfer Learning to Defect Graph Neural Networks 
                  for Defect Formation Energy Predictions</td>
                <td>May&nbsp;2024 to July&nbsp;2024</td>
                <td>Poster, graduate&nbsp;assistant</td> 
                <td>Submitted, waiting</td> 
              </tr> 
              <tr> 
                <td>Deputy NoC: A Case of Low Cost Network-on-Chip for Neural 
                  Network Accelerations on GPUs</td>
                <td>August&nbsp;2023 to May&nbsp;2024</td> 
                <td>Paper, coauthor</td> 
                <td>Submitted, waiting</td> 
              </tr>
              <tr> 
                <td>Elastic-Float: Lossy Cache Compression for Cost Effective
                  Neural Network Acceleration</td> 
                <td>January&nbsp;2023 to December&nbsp;2023</td> 
                <td>Paper, coauthor</td>
                <td>Rejected, revising</td>
              </tr>
              <tr>
                <td>Genomics-GPU: A Benchmark Suite for GPU-accelerated Genome 
                  Analysis</td> 
                <td>August&nbsp;2022 to November&nbsp;2022</td> 
                <td>Paper, coauthor</td>
                <td>Accepted, ISPASS&nbsp;2023</td>
              </tr>
              <tr> 
                <td>GPU Implementation of Image Recognition Neural Network 
                  Architectures</td> 
                <td>May&nbsp;2022 to July&nbsp;2022</td> 
                <td>Poster, main&nbsp;author</td> 
                <td>Presented, UNT&nbsp;2022&nbsp;REU</td> 
              </tr> 
            </table> 
          </p>
        </div> 

        <div class="main-section"> 
          <p class="main-text-h2">Reducing the Amount of Duplicated Values in 
            the GPU L2 Cache during Lowered Convolution.</p> 
          <p class="main-text">Status: in progress</p> 
          <p class="main-text">Timeline: April 2023 to current</p> 
          <p class="main-text">
            For the 2d-convolution operation used frequently in image 
            recognition, we can convert it into a matrix multiplication by
            duplicating the data in input/weight matrices in a specific way. 
            Although the duplication makes the cache hierarchy less useful, 
            since a cache miss might not have occurred if the mapping function 
            was slightly different. We partition the L2 cache into "unique"
            and "normal" parts and use a new mapping function that minimizes 
            duplicated values in the unique section if the passed address is 
            within the bounds of the lowered matrix. This is my thesis topic!
          </p>
          <details> 
            <summary class="main-text-details">Specific contributions</summary> 
            <ul class="main-text-list-indent"> 
              <li>Made many different changes to the GPGPU-Sim simulator, 
                including to the memory system, interconnect method, and timing
                simulator.</li>
              <li>Implemented motivational tests that show how much duplication 
                was present at different timesteps of the program for different
                convolution layer sizes found throughout image recognition.
                </li> 
              <li>First time leading a full research project!</li>
            </ul>
          </details> 
        </div> 

        <div class="main-section"> 
          <p class="main-text-h2">Using Transfer Learning and Supercells to 
            Improve Graph Neural Network Performance in Formation Energy 
            Predictions</p> 
          <p class="main-text">Status: in-progress</p> 
          <p class="main-text">Timeline: May 2024 to current</p> 
          <p class="main-text">
            Expands on previous work to use supercells from density functional 
            theory to represent the defect crystal input to a graph neural 
            network. Each crystal material is normally represented as a 
            non-repeating "unit cell" with each atom as a node and each bond as 
            an edge, while <i>supercells</i> contain repetitions of unit cells 
            that are connected to each other with a pooling function that 
            collates the nodes from different repeated instances. Uses transfer 
            learning to support a much larger graph, avoiding a problem other
            work faced when trying to increase model size with such a small
            dataset.
          </p>
          <details> 
            <summary class="main-text-details">Specific contributions</summary> 
            <ul class="main-text-list-indent"> 
              <li>Expanded <a href="https://github.com/txie-93/cgcnn">CGCNN</a>
                to train a network on supercells containing duplicated unit 
                cells.</li>
              <li>Used creative learning strategies (including genetic 
                hyperparameter optimizers and circular learning rate methods 
                that work especially well for transfer learning) to obtain a 
                higher performance.</li> 
              <li>Worked with a physics professor to show how changes to the 
                deep-learning model are still physically logical.
            </ul>
          </details>
        </div> 

        <div class="main-section">
          <p class="main-text-h2">Applying Transfer Learning to Defect Graph 
            Neural Networks for Defect Formation Energy Predictions</p>
          <p class="main-text">Status: submitted, waiting
            (<a href="./media/transfer-defect-poster.pdf">poster</a>, 
            <a href="./media/transfer-defect-report.pdf">report</a>)</p>
          <p class="main-text">Timeline: May 2024 to July 2024</p> 
          <p class="main-text"> 
            Showed how transfer learning could be utilized to improve the
            performance of a graph neural network pretrained on pristine 
            crystal structures and post-trained to predict the formation 
            energies of defected crystal structures. The defect dataset is very 
            small while the pristine dataset is comparatively a lot larger, so 
            transfer learning was shown to improve the performance by a small 
            amount. Worked with two undergraduate students during UNT's Summer 
            2024 Artificial Intelligence REU.
          </p> 
          <details> 
            <summary class="main-text-details">Specific contributions</summary> 
            <ul class="main-text-list-indent"> 
              <li>Led the undergraduate students and organized meetings to 
                ensure the success of the project, assigning specific tasks for 
                each student to complete and working with the professor and 
                another graduate student regularly.</li>
              <li>Modified the <a href=
                "https://github.com/justinmgarrigus/cgcnn">CGCNN</a> and 
                <a href="https://github.com/justinmgarrigus/cgcnndefect">dGNN
                </a> source code to implement transfer learning.</li>
              <li>Collected figures that demonstrate the effects of modifying 
                different hyperparameters (like layer size, number of layers, 
                training method, etc.) on the final model performance.</li> 
            </ul>
          </details>
        </div> 

        <div class="main-section"> 
          <p class="main-text-h2">Deputy NoC: A Case of 
            Low Cost Network-on-Chip for Neural Network Accelerations on GPUs
            </p>
          <p class="main-text">Status: submitted, waiting</p>
          <p class="main-text">Timeline: August 2023 to May 2024</p> 
          <p class="main-text">
            Improves the power consumption and performance of deep neural 
            networks by changing the GPU's network-on-chip to exploit data 
            locality in the exponent field and only sending redundant exponents 
            once, while using "deputy values" to further compress the mantissa 
            field.
          </p> 
          <details> 
            <summary class="main-text-details">Specific contributions</summary> 
            <ul class="main-text-list-indent"> 
              <li>Uses the same <a href=
                "https://github.com/justinmgarrigus/layer-simulation">Python 
                and C++ projects</a> below adapted with new functionality, 
                especially in regards to the layout of the input and weight 
                tensors for each convolution function.</li>
              <li>Made small changes to the GPU simulator and the configuration 
                files with the help of the project lead.</li>
              <li>Organized tests and formatted data to be used as figures.
                </li>
            </ul>
          </details>
        </div> 
    
        <div class="main-section"> 
          <p class="main-text-h2">Elastic-Float: Lossy 
            Cache Compression for Cost Effective Neural Network Acceleration
            </p>
          <p class="main-text">Status: rejected, revising</p> 
          <p class="main-text">Timeline: January 2023 to December 2023</p>
          <p class="main-text">
            Developed a cache compression method for deep neural networks that
            significantly lowers cache power and capacity without harming 
            performance or accuracy by exploiting value locality in 
            floating-point data. I maintained the <a href=
            "https://github.com/justinmgarrigus/layer-simulation">Python and 
            C++ projects</a> that were used as benchmarks to measure the 
            performance of the methods, along with collecting the data for some 
            figures. 
          </p>
          <details> 
            <summary class="main-text-details">Specific contributions</summary> 
            <ul class="main-text-list-indent"> 
              <li>Adapted CUTLASS code written by a previous student to improve
                its versatility for our project.</li> 
              <li>Deconstructed famous image-recognition networks like AlexNet
                and ResNet into base functions that we can run independently on 
                our GPU simulator.</li>
              <li>Created <a href=
                "https://github.com/justinmgarrigus/layer-simulation">Python 
                code</a> that executes each of these base functions, turning 
                one or more images into a list of image classifications.</li> 
              <li>Ran tests both locally and on the Texas Advanced Computing 
                Center (TACC) <a href=
                "https://tacc.utexas.edu/systems/lonestar6/">Lonestar6</a> 
                supercomputer.</li>
            </ul> 
          </details> 
        </div>

        <div class="main-section"> 
          <p class="main-text-h2">Genomics-GPU: A 
            Benchmark Suite for GPU-accelerated Genome Analysis</p>
          <p class="main-text">Status: published, <a href=
              "https://ieeexplore.ieee.org/abstract/document/10158156">ISPASS 
              2023</a></p>
          <p class="main-text">Timeline: August 2022 to November 2022</p>
          <p class="main-text">
            Created a benchmark suite for studying the effects of 
            microarchitectural changes to the GPU for genome analysis. I 
            adapted the <a href=
            "https://github.com/justinmgarrigus/gene-sequence-clustering-CDP">
            gene-clustering benchmark</a> to use CUDA dynamic parallelism and 
            obtained a 24.2% improvement in performance. I also ran several 
            tests on the simulator to collect data for figures. This was my 
            first experience assisting with a research project! 
          </p>
          <details> 
            <summary class="main-text-details">Specific contributions</summary> 
            <ul class="main-text-list-indent">
              <li>Refactored the general algorithm from the <a href=
                "https://github.com/justinmgarrigus/gene-sequence-clustering-CDP"
                >gene-clustering algorithm</a> to use CUDA dynamic parallelism, 
                allowing the program to complete in 24.2% of the time on 
                average.</li>
              <li>Aided in the compilation and execution of benchmarks like 
                <a href="https://github.com/nahmedraja/GASAL2">GASAL2</a> on 
                the GPU simulator by modifying the source code.</li> 
              <li>Collected the data for simple tests under close supervision 
                of the PhD students involved.</li> 
              <li>Learned how GPGPU-Sim (and processor-simulators in general) 
                work.</li> 
            </ul> 
          </details>  
        </div> 

        <div class="main-section"> 
          <p class="main-text-h2">GPU Implementation of 
            Image Recognition Neural Network Architectures</p>
          <p class="main-text">Status: presented, UNT 2022 REU
            (<a href="./media/gpu-cpu-image-recognition-poster.pdf">poster</a>, 
            <a href="./media/gpu-cpu-image-recognition-report.pdf">report</a>)
            </p>
          <p class="main-text">Timeline: May 2022 to July 2022</p>
          <p class="main-text"> 
            Worked with another undergraduate student to compare the 
            performance difference in using the CPU versus the GPU in doing 
            image classification with the AlexNet, ResNet, and VGG-16 
            architectures. This involved re-creating each operator in each 
            model in CUDA (both with and without tensor-cores) and Python.
          </p> 
          <details> 
            <summary class="main-text-details">Specific contributions</summary>
            <ul class="main-text-list-indent">
              <li>Collaborated with another student for the first time in a 
                research setting.</li> 
              <li>Implemented deep learning operators like 2D convolution,
                matrix multiplication, max pool, ReLU, and Softmax in Python
                (with Keras), C, and CUDA (with and without tensorcores).</li>
              <li>Recreated AlexNet, ResNet, and VGG-16 in each language by 
                closely following thier papers.</li>
            </ul>
          </details>
        </div> 

        <div class="main-section">
          <p class="main-text-h2">GPU Implementation of 
            Image Recognition Neural Network Architectures</p>
          <p class="main-text">Status: presented, UNT 2022 REU
            (<a href="./media/gpu-cpu-image-recognition-poster.pdf">poster</a>, 
            <a href="./media/gpu-cpu-image-recognition-report.pdf">report</a>)
            </p>
          <p class="main-text">Timeline: May 2022 to July 2022</p>
          <p class="main-text"> 
            Worked with another undergraduate student to compare the 
            performance difference in using the CPU versus the GPU in doing 
            image classification with the AlexNet, ResNet, and VGG-16 
            architectures. This involved re-creating each operator in each 
            model in CUDA (both with and without tensor-cores) and Python.
          </p> 
          <details> 
            <summary class="main-text-details">Specific contributions</summary>
            <ul class="main-text-list-indent">
              <li>Collaborated with another student for the first time in a 
                research setting.</li> 
              <li>Implemented deep learning operators like 2D convolution,
                matrix multiplication, max pool, ReLU, and Softmax in Python
                (with Keras), C, and CUDA (with and without tensorcores).</li>
              <li>Recreated AlexNet, ResNet, and VGG-16 in each language by 
                closely following thier papers.</li>
            </ul>
          </details>
        </div>
      </div>

      <div class="last-updated"> 
        <p class="last-updated-text">
          Last updated August 4, 2024 at 9:22am CST
       </p>
      </div>  

    </div>

    <div class="sidebar">
      <img src="../profile-pic.jpg" alt="Profile Picture" class="profile-pic">
      <p class="sidebar-text"><a href="../index.html">Home</a></p>
      <p class="sidebar-text"><a href="../cv/index.html">CV</a></p> 
      <p class="sidebar-text"><a href="./index.html">Research</a></p>
      <p class="sidebar-text"><a href="../projects/index.html">Projects</a></p> 
      <p class="sidebar-text"><a href="../teaching/index.html">Teaching</a></p> 
      <p class="sidebar-text"><a href="../cranes/index.html">Cranes</a></p>
    </div>
  
  </body>
</html>
